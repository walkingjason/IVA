{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "def get_fib_seq(n, scale_data=True):\n",
    "    # Get the Fibonacci sequence\n",
    "    seq = np.zeros(n)\n",
    "    fib_n1 = 0.0\n",
    "    fib_n = 1.0 \n",
    "    for i in range(n):\n",
    "            seq[i] = fib_n1 + fib_n\n",
    "            fib_n1 = fib_n\n",
    "            fib_n = seq[i] \n",
    "    scaler = []\n",
    "    if scale_data:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        seq = np.reshape(seq, (n, 1))\n",
    "        seq = scaler.fit_transform(seq).flatten()        \n",
    "    return seq, scaler\n",
    " \n",
    "fib_seq = get_fib_seq(10, False)[0]\n",
    "# print(fib_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train and test data\n",
    "def get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n",
    "    dat, scaler = get_fib_seq(total_fib_numbers, scale_data)    \n",
    "    Y_ind = np.arange(time_steps, len(dat), 1)\n",
    "    Y = dat[Y_ind]\n",
    "    rows_x = len(Y)\n",
    "    X = dat[0:rows_x]\n",
    "    for i in range(time_steps-1):\n",
    "        temp = dat[i+1:rows_x+i+1]\n",
    "        X = np.column_stack((X, temp))\n",
    "    # random permutation with fixed seed   \n",
    "    rand = np.random.RandomState(seed=13)\n",
    "    idx = rand.permutation(rows_x)\n",
    "    split = int(train_percent*rows_x)\n",
    "    train_ind = idx[0:split]\n",
    "    test_ind = idx[split:]\n",
    "    trainX = X[train_ind]\n",
    "    trainY = Y[train_ind]\n",
    "    testX = X[test_ind]\n",
    "    testY = Y[test_ind]\n",
    "    trainX = np.reshape(trainX, (len(trainX), time_steps, 1))    \n",
    "    testX = np.reshape(testX, (len(testX), time_steps, 1))\n",
    "    return trainX, trainY, testX, testY, scaler\n",
    " \n",
    "trainX, trainY, testX, testY, scaler = get_fib_XY(12, 3, 0.7, False)\n",
    "# print('trainX = ', trainX)\n",
    "# print('trainY = ', trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the network without attention\n",
    "# Set up parameters\n",
    "time_steps = 20\n",
    "hidden_units = 2\n",
    "epochs = 30\n",
    " \n",
    "# Create a traditional RNN network\n",
    "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n",
    "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    " \n",
    "model_RNN = create_RNN(hidden_units=hidden_units, dense_units=1, input_shape=(time_steps,1), \n",
    "                   activation=['tanh', 'tanh'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "def create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n",
    "    x=Input(shape=input_shape)\n",
    "    RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n",
    "    attention_layer = attention()(RNN_layer)\n",
    "    outputs=Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n",
    "    model=Model(x,outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')    \n",
    "    return model    \n",
    " \n",
    "model_attention = create_RNN_with_attention(hidden_units=hidden_units, dense_units=1, \n",
    "                                  input_shape=(time_steps,1), activation='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "826/826 - 13s - loss: 0.0037 - 13s/epoch - 16ms/step\n",
      "Epoch 2/30\n",
      "826/826 - 12s - loss: 0.0033 - 12s/epoch - 14ms/step\n",
      "Epoch 3/30\n",
      "826/826 - 10s - loss: 0.0029 - 10s/epoch - 12ms/step\n",
      "Epoch 4/30\n",
      "826/826 - 11s - loss: 0.0026 - 11s/epoch - 13ms/step\n",
      "Epoch 5/30\n",
      "826/826 - 10s - loss: 0.0022 - 10s/epoch - 12ms/step\n",
      "Epoch 6/30\n",
      "826/826 - 11s - loss: 0.0019 - 11s/epoch - 13ms/step\n",
      "Epoch 7/30\n",
      "826/826 - 11s - loss: 0.0017 - 11s/epoch - 13ms/step\n",
      "Epoch 8/30\n",
      "826/826 - 11s - loss: 0.0015 - 11s/epoch - 13ms/step\n",
      "Epoch 9/30\n",
      "826/826 - 9s - loss: 0.0013 - 9s/epoch - 11ms/step\n",
      "Epoch 10/30\n",
      "826/826 - 8s - loss: 0.0012 - 8s/epoch - 10ms/step\n",
      "Epoch 11/30\n",
      "826/826 - 9s - loss: 0.0011 - 9s/epoch - 11ms/step\n",
      "Epoch 12/30\n",
      "826/826 - 10s - loss: 9.8978e-04 - 10s/epoch - 12ms/step\n",
      "Epoch 13/30\n",
      "826/826 - 9s - loss: 9.2920e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 14/30\n",
      "826/826 - 8s - loss: 8.8509e-04 - 8s/epoch - 10ms/step\n",
      "Epoch 15/30\n",
      "826/826 - 9s - loss: 8.2872e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 16/30\n",
      "826/826 - 10s - loss: 7.9395e-04 - 10s/epoch - 12ms/step\n",
      "Epoch 17/30\n",
      "826/826 - 10s - loss: 7.5559e-04 - 10s/epoch - 12ms/step\n",
      "Epoch 18/30\n",
      "826/826 - 8s - loss: 7.1396e-04 - 8s/epoch - 10ms/step\n",
      "Epoch 19/30\n",
      "826/826 - 9s - loss: 6.6904e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 20/30\n",
      "826/826 - 8s - loss: 6.4677e-04 - 8s/epoch - 10ms/step\n",
      "Epoch 21/30\n",
      "826/826 - 9s - loss: 6.0730e-04 - 9s/epoch - 10ms/step\n",
      "Epoch 22/30\n",
      "826/826 - 8s - loss: 5.8004e-04 - 8s/epoch - 10ms/step\n",
      "Epoch 23/30\n",
      "826/826 - 8s - loss: 5.4183e-04 - 8s/epoch - 10ms/step\n",
      "Epoch 24/30\n",
      "826/826 - 9s - loss: 5.1289e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 25/30\n",
      "826/826 - 9s - loss: 4.8414e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 26/30\n",
      "826/826 - 9s - loss: 4.5310e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 27/30\n",
      "826/826 - 10s - loss: 4.2163e-04 - 10s/epoch - 12ms/step\n",
      "Epoch 28/30\n",
      "826/826 - 9s - loss: 3.9409e-04 - 9s/epoch - 10ms/step\n",
      "Epoch 29/30\n",
      "826/826 - 8s - loss: 3.6309e-04 - 8s/epoch - 10ms/step\n",
      "Epoch 30/30\n",
      "826/826 - 8s - loss: 3.3851e-04 - 8s/epoch - 10ms/step\n",
      "Epoch 1/30\n",
      "826/826 - 11s - loss: 0.0014 - 11s/epoch - 13ms/step\n",
      "Epoch 2/30\n",
      "826/826 - 8s - loss: 0.0014 - 8s/epoch - 10ms/step\n",
      "Epoch 3/30\n",
      "826/826 - 8s - loss: 0.0014 - 8s/epoch - 10ms/step\n",
      "Epoch 4/30\n",
      "826/826 - 8s - loss: 0.0014 - 8s/epoch - 10ms/step\n",
      "Epoch 5/30\n",
      "826/826 - 9s - loss: 0.0013 - 9s/epoch - 10ms/step\n",
      "Epoch 6/30\n",
      "826/826 - 9s - loss: 0.0013 - 9s/epoch - 11ms/step\n",
      "Epoch 7/30\n",
      "826/826 - 9s - loss: 0.0013 - 9s/epoch - 11ms/step\n",
      "Epoch 8/30\n",
      "826/826 - 8s - loss: 0.0013 - 8s/epoch - 10ms/step\n",
      "Epoch 9/30\n",
      "826/826 - 9s - loss: 0.0013 - 9s/epoch - 11ms/step\n",
      "Epoch 10/30\n",
      "826/826 - 9s - loss: 0.0012 - 9s/epoch - 11ms/step\n",
      "Epoch 11/30\n",
      "826/826 - 9s - loss: 0.0012 - 9s/epoch - 11ms/step\n",
      "Epoch 12/30\n",
      "826/826 - 9s - loss: 0.0012 - 9s/epoch - 11ms/step\n",
      "Epoch 13/30\n",
      "826/826 - 11s - loss: 0.0011 - 11s/epoch - 13ms/step\n",
      "Epoch 14/30\n",
      "826/826 - 9s - loss: 0.0011 - 9s/epoch - 11ms/step\n",
      "Epoch 15/30\n",
      "826/826 - 9s - loss: 0.0010 - 9s/epoch - 11ms/step\n",
      "Epoch 16/30\n",
      "826/826 - 9s - loss: 0.0010 - 9s/epoch - 11ms/step\n",
      "Epoch 17/30\n",
      "826/826 - 9s - loss: 9.5841e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 18/30\n",
      "826/826 - 9s - loss: 9.0065e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 19/30\n",
      "826/826 - 9s - loss: 8.5457e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 20/30\n",
      "826/826 - 9s - loss: 7.7895e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 21/30\n",
      "826/826 - 9s - loss: 7.3001e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 22/30\n",
      "826/826 - 8s - loss: 6.5910e-04 - 8s/epoch - 10ms/step\n",
      "Epoch 23/30\n",
      "826/826 - 9s - loss: 5.9625e-04 - 9s/epoch - 10ms/step\n",
      "Epoch 24/30\n",
      "826/826 - 9s - loss: 5.3170e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 25/30\n",
      "826/826 - 9s - loss: 4.5583e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 26/30\n",
      "826/826 - 9s - loss: 3.9710e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 27/30\n",
      "826/826 - 9s - loss: 3.4177e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 28/30\n",
      "826/826 - 9s - loss: 2.8995e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 29/30\n",
      "826/826 - 9s - loss: 2.3949e-04 - 9s/epoch - 11ms/step\n",
      "Epoch 30/30\n",
      "826/826 - 9s - loss: 2.0131e-04 - 9s/epoch - 11ms/step\n",
      "26/26 [==============================] - 1s 8ms/step - loss: 2.9321e-04\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.7043e-04\n",
      "26/26 [==============================] - 1s 9ms/step - loss: 1.5438e-04\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 5.1856e-05\n",
      "Train set MSE =  0.0002932118659373373\n",
      "Test set MSE =  0.0001704306632746011\n",
      "Train set MSE with attention =  0.00015438332047779113\n",
      "Test set MSE with attention =  5.1855877245543525e-05\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "# Generate the dataset\n",
    "trainX, trainY, testX, testY, scaler  = get_fib_XY(1200, time_steps, 0.7)\n",
    " \n",
    "model_RNN.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
    "model_attention.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
    " \n",
    "# Evalute model\n",
    "train_mse = model_RNN.evaluate(trainX, trainY)\n",
    "test_mse = model_RNN.evaluate(testX, testY)\n",
    "train_mse_attn = model_attention.evaluate(trainX, trainY)\n",
    "test_mse_attn = model_attention.evaluate(testX, testY)\n",
    "\n",
    "# Print error\n",
    "print(\"Train set MSE = \", train_mse)\n",
    "print(\"Test set MSE = \", test_mse)\n",
    "print(\"Train set MSE with attention = \", train_mse_attn)\n",
    "print(\"Test set MSE with attention = \", test_mse_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c391d0701481a57735b45bc97b9001b45bf3c058120e47b531dd564758aaf30"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
